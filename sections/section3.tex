\section{The TensorFlow Programming Interface}\label{sec:code}

Having conveyed the abstract concepts of TensorFlow's computational model in
Section \ref{sec:model}, we will now concretize those ideas and speak to
TensorFlow's programming interface. In the following paragraphs we give a
step-by-step walkthrough of a practical, real-world example of TensorFlow's
Python API. We will train a simple multi-layer perceptron (MLP) with one input
and one output layer to classify handwritten digits in the
MNIST\footnote{http://yann.lecun.com/exdb/mnist/} dataset. In this dataset, the
examples are small images of $28 \times 28$ pixels depicting handwritten digits
in $\in \{0, \dots, 9\}$. We receive each such example as a flattened vector of
$28 \times 28 = 784$ gray-scale pixel intensities. The label for each example is
a \emph{one-hot-encoded} vector $(d_1, \dots, d_{10})^\top$ where the $i$-th
component is set to one while all others are zero if the image shows the digit
$i$.

We begin our walkthrough by importing the TensorFlow library:
%
\begin{lstlisting}
import tensorflow as tf
\end{lstlisting}
%
Next, we create a new computational graph via the \texttt{tf.Graph}
constructor. To add operations to this graph, we must register it as the
\emph{default graph}. The way the TensorFlow API is designed, library routines
that create new operation nodes always attach these to the current default
graph. We register our graph as the default by using it as a Python context
manager in a \texttt{with-as} statement:
%
\begin{lstlisting}
# Create a new graph
graph = tf.Graph()

# Register the graph as the default one to add nodes
with graph.as_default():
  # Add operations ...
\end{lstlisting}
%
We are now ready to populate our computational graph with operations. We begin
by adding two \emph{placeholder} nodes \texttt{examples} and
\texttt{labels}. Placeholders are special variables that \emph{must} be replaced
with concrete tensors upon graph execution. That is, they must be supplied in
the \texttt{feed\_dict} argument to \texttt{Session.run()}, mapping tensors to
replacement values. For each such placeholder, we specify a shape and data
type. By specifying the keyword \texttt{None} for the first dimension of each
placeholder shape, we can later feed a tensor of variable size in that
dimension. For the column size of the \texttt{example} placeholder, we specify
the number of features for each image, meaning the $28 \times 28 = 784$
pixels. The label placeholder should expect 10 columns, corresponding to the
10-dimensional one-hot-encoded vector for each label digit:
%
\begin{lstlisting}
# Using 32-bit floating-point data type tf.float32
examples = tf.placeholder(tf.float32, [None, 784])
labels = tf.placeholder(tf.float32, [None, 10])
\end{lstlisting}
%
Given an example matrix $\mathbf{X} \in \mathbb{R}^{n \times 784}$ containing
$n$ images, the learning task then applies an affine transformation
$\mathbf{X} \cdot \mathbf{W} + \mathbf{b}$, where $\mathbf{W}$ is a
\emph{weight} matrix $\in \mathbb{R}^{784 \times 10}$ and $\mathbf{b}$ a
\emph{bias} vector $\in \mathbb{R}^{10}$. This yields a new matrix
$\mathbf{Y} \in \mathbb{R}^{n \times 10}$, containing the \emph{scores} or
\emph{logits} of our model for each example and each possible digit. To
transform the logits into a valid probability distribution, we apply the
\emph{softmax} function:
%
\begin{lstlisting}
# Draw random weights for symmetry breaking
weights = tf.Variable(tf.random_uniform([784, 10]))
# Slightly positive initial bias
bias = tf.Variable(tf.constant(0.1, shape=[10]))
# tf.matmul performs the matrix multiplication XW
# Note how the + operator is overloaded for tensors
logits = tf.matmul(examples, weights) + bias
# Applies the operation element-wise on tensors
estimates = tf.nn.softmax(logits)
\end{lstlisting}
%
We then compute our objective function, producing the error or \emph{loss} of
the model given its current trainable parameters $\mathbf{W}$ and
$\mathbf{b}$. We do this by calculating the \emph{cross entropy}
$H(\mathbf{L}, \mathbf{Y})_i = -\sum_j \mathbf{L}_{i,j} \cdot
\log(\mathbf{Y}_{i,j})$ between the probability distributions of our estimates
$\mathbf{Y}$ and the one-hot-encoded labels $\mathbf{L}$. More precisely, we
consider the mean cross entropy over all examples as the loss:
%
\begin{lstlisting}
# Computes the cross-entropy and sums the rows
cross_entropy = -tf.reduce_sum(
    labels * tf.log(estimates), [1])
loss = tf.reduce_mean(cross_entropy)
\end{lstlisting}
%
Now that we have an objective function, we can run (stochastic) gradient descent
to update the weights of our model. For this, TensorFlow provides a
\texttt{GradientDescentOptimizer} class. It is initialized with the
learning rate of the algorithm and provides an operation \texttt{minimize}, to
which we pass our \texttt{loss} tensor. This is the operation we will run
repeatedly in a \texttt{Session} environment to train our model:
%
\begin{lstlisting}
# We choose a learning rate of 0.5
gdo = tf.train.GradientDescentOptimizer(0.5)
optimizer = gdo.minimize(loss)
\end{lstlisting}
%
Finally, we can actually train our algorithm. For this, we enter a session
environment using a \texttt{tf.Session} as a context manager. We pass our graph
object to its constructor, so that it knows which graph to manage. To then
execute nodes, we use \texttt{Session.run()} and pass a list of tensors we wish
to compute. Before evaluating any other node, we must first initialize the
variables in our graph. We do this by \texttt{run}ning the
\texttt{tf.initialize\_all\_variables()} utility operation. Then, we perform a
certain number of iterations of stochastic gradient descent, fetching an example
and label mini-batch from the MNIST dataset each time and feeding it to the
\texttt{run} routine. For this, we assume a utility object \texttt{mnist} which
can be used as shown below. At the end, our loss will (hopefully) be small:
%
\begin{lstlisting}
with tf.Session(graph=graph) as session:
    # Execute the operation directly
    tf.initialize_all_variables().run()
    for step in range(1000):
        # Fetch next 100 examples and labels
        x, y = mnist.train.next_batch(100)
        # Ignore the result of the optimizer (None)
        _, loss_value = session.run(
            [optimizer, loss],
            feed_dict={examples: x, labels: y})
        print('Loss at step {0}: {1}'
                .format(step, loss_value))
\end{lstlisting}
%
The full code listing for this example, along with some additional
implementation to compute an accuracy metric at each time step is given in
Appendix \ref{app:code}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: